# -*- coding: utf-8 -*-
"""datamerging.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Moy4H63d7ZNdJVnUks_KANuQ7BAbXRS

# Merging all data sources
"""
import os

import pandas as pd
import matplotlib.pyplot as plt
import datetime
from datetime import timedelta
import numpy as np
import torch.cuda

"""Load all the previously processed data from the source files in the MIMIC4 dataset. <br/>Unify column naming across data sources and merge to one dataframe."""

import torch
import os

pd.set_option('display.max_rows', 50)
pd.set_option('display.max_columns', 300)

data_path = 'nfe/experiments/data/physionet.org/files/mimiciv/2.2'

hos_path = f'{data_path}/hosp'
icu_path = f'{data_path}/icu'
out_path = 'nfe/experiments/data'
proc_path = f'{out_path}/mimic4_processed'
os.makedirs(proc_path, exist_ok=True)

lab_df=pd.read_csv(f"{proc_path}/lab_processed.csv")[["subject_id","hadm_id","charttime","valuenum","label"]]
inputs_df=pd.read_csv(f"{proc_path}/inputs_processed.csv")[["subject_id","hadm_id","charttime","amount","label"]]
outputs_df=pd.read_csv(f"{proc_path}/outputs_processed.csv")[["subject_id","hadm_id","charttime","value","label"]]
presc_df=pd.read_csv(f"{proc_path}/prescriptions_processed.csv")[["subject_id","hadm_id","charttime","dose_val_rx","drug"]]

#Change the name of amount. Valuenum for every table
inputs_df["valuenum"]=inputs_df["amount"]
inputs_df.head()
inputs_df=inputs_df.drop(columns=["amount"]).copy()

outputs_df["valuenum"]=outputs_df["value"]
outputs_df=outputs_df.drop(columns=["value"]).copy()

presc_df["valuenum"]=presc_df["dose_val_rx"]
presc_df=presc_df.drop(columns=["dose_val_rx"]).copy()
presc_df["label"]=presc_df["drug"]
presc_df=presc_df.drop(columns=["drug"]).copy()
presc_df = presc_df.drop((presc_df['valuenum']=='3-10').index)

#Tag to distinguish between lab and inputs events
inputs_df["Origin"]="Inputs"
lab_df["Origin"]="Lab"
outputs_df["Origin"]="Outputs"
presc_df["Origin"]="Prescriptions"

#merge both dfs.
merged_df1=(inputs_df.append(lab_df)).reset_index()
merged_df2=(merged_df1.append(outputs_df)).reset_index()
merged_df2.drop(columns="level_0",inplace=True)
merged_df=(merged_df2.append(presc_df)).reset_index()

#Check that all labels have different names.
assert(merged_df["label"].nunique()==(inputs_df["label"].nunique()+lab_df["label"].nunique()+outputs_df["label"].nunique()+presc_df["label"].nunique()))

# set the timestamp as the time delta between the first chart time for each admission
merged_df['charttime']=pd.to_datetime(merged_df["charttime"], format='%Y-%m-%d %H:%M:%S')
ref_time=merged_df.groupby("hadm_id")["charttime"].min()
merged_df_1=pd.merge(ref_time.to_frame(name="ref_time"),merged_df,left_index=True,right_on="hadm_id")
merged_df_1["time_stamp"]=merged_df_1["charttime"]-merged_df_1["ref_time"]
assert(len(merged_df_1.loc[merged_df_1["time_stamp"]<timedelta(hours=0)].index)==0)

# Create a label code (int) for the labels.
label_dict=dict(zip(list(merged_df_1["label"].unique()),range(len(list(merged_df_1["label"].unique())))))
merged_df_1["label_code"]=merged_df_1["label"].map(label_dict)

merged_df_short=merged_df_1[["hadm_id","valuenum","time_stamp","label_code","Origin"]]

label_dict_df=pd.Series(merged_df_1["label"].unique()).reset_index()
label_dict_df.columns=["index","label"]
label_dict_df["label_code"]=label_dict_df["label"].map(label_dict)
label_dict_df.drop(columns=["index"],inplace=True)
label_dict_df.to_csv(f"{proc_path}/label_dict.csv")

merged_df_short["valuenum"] = merged_df_short["valuenum"].astype(float)

# select only values within first 48 hours
merged_df_short=merged_df_short.loc[(merged_df_short["time_stamp"]<timedelta(hours=48))]
merged_df_short["time_stamp"] = merged_df_short["time_stamp"].dt.total_seconds().div(60).astype(int)
print("Number of patients considered: "+str(merged_df_short["hadm_id"].nunique()))
assert(len(merged_df_short.loc[merged_df_short["time_stamp"]>2880].index)==0)

# drop columns that are not needed for final dataset
merged_df_short.drop(["Origin"], axis=1, inplace=True)
complete_df = merged_df_short

# create value- and mask- columns and fill with data
labels = complete_df["label_code"].unique()
value_columns = []
mask_columns  = []
print(complete_df.columns.values)
for num in labels:
    name = "Value_label_" + str(num)
    name2 = "Mask_label_" + str(num)
    value_columns.append(name)
    mask_columns.append(name2)
    complete_df[name] = 0
    complete_df[name2] = 0
    complete_df[name] = complete_df[name].astype(float)

print(complete_df.columns.values)
complete_df.dropna(inplace=True)
for index, row in complete_df.iterrows():
    name = "Value_label_" + str(row["label_code"].astype(int))
    name2 = "Mask_label_" + str(row["label_code"].astype(int))
    complete_df.at[index, name] = row["valuenum"]
    complete_df.at[index, name2] = 1
print(complete_df.columns.values)
# drop all unneccesary columns and do sanity check
complete_df.drop(["valuenum", "label_code"], axis=1, inplace=True)
complete_df = complete_df.groupby(["hadm_id", "time_stamp"], as_index=False).max()
for x in mask_columns:
    assert(len(complete_df.loc[complete_df[x]>1])==0)
complete_df

cuda = torch.cuda.is_available()
nrows = 50
import random

# Define a function to create a random integer of length n
def create_id(n):
    range_start = 10**(n-1)
    range_end = (10**n)-1
    return str(random.randint(range_start, range_end))

if not cuda:
    unique_ids = set()
    while len(unique_ids) < nrows:
        unique_ids.add(create_id(8))
    complete_df = complete_df.head(nrows)
    complete_df['hadm_id'] = list(unique_ids)

print(complete_df.head())

out_path='nfe/experiments/data'
complete_df.to_csv(f"{proc_path}/full_dataset.csv", index=False)
FINAL_PATH = f'{out_path}/mimic4'
os.makedirs(f'{FINAL_PATH}', exist_ok=True)
complete_df.to_csv(f"{FINAL_PATH}/mimic4_full_dataset.csv", index=False)

print('done')

